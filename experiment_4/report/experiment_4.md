# 《神经网络与深度学习》课程实验作业（四）

**实验内容：自然语言处理**

注意事项：

①  本次实验包含一道题，共计30分；

②  所有实验结果需以实验报告的形式进行提交，文件命名格式：实验四_姓名_学号.pdf，文件中需要将作者设置为本人姓名；

③  实验报告中可插入代码片段，完整代码无需放在实验报告中，以压缩包的形式添加即可，压缩包命名格式：实验四_姓名_学号.zip；

④  作业提交截止时间：2025年12月31日晚上23：59

搭建Transformer编码器完成文本语义匹配任务(30分)

AFQMC数据集是一个蚂蚁金融语义相似度数据集，用于问题相似度计算，数据集包括训练集、验证集、测试集（无label，本实验不使用）3个文件，分别包含34334、4316以及3861条数据，每条数据有三个属性，分别是句子1、句子2、句子相似度标签。相似度标签为1表示两个句子含义类似，标签为0则表示含义不同。请基于该数据集完成以下实验内容：

(1)   数据集构建，包括：利用词表将句子中的每个中文字符转换成id、对不在词汇表里面的字做出适当处理、在输入中加入句子的分隔符号、在起始位置加入占位符、小批量数据的组装及对齐。构建完成后打印一条mini-batch的数据进行验证。 （3分）

(2)   实现输入编码、分段编码和位置编码，并组装为嵌入层，打印该层的输入输出。（3分）

(3)   实现多头自注意力层和add&norm层。 （3分）

(4)   搭建一个transformer编码器，利用嵌入层、transformer编码器和合适的分类器构建完成你的语义匹配模型，并说明你的模型组成，可画图说明。 （3分）

(5)   训练模型，在验证集上计算准确率，并保存在验证集上准确率最高的模型 （3分） 使用tensorboard等可视化插件，展示训练过程中的精度变化和损失变化。 （4分）

(6)   加载保存的模型，在验证集上随机选取50条数据进行语义匹配测试，展示模型的预测结果。 （3分）

(7)   输入一条样本提取多头注意力权重，对注意力机制的计算结果进行可视化展示（效果如下）（3分）并分析（1分）。

![img](./1.jpg)

(8)   改变transformer的层数再次实验，输出测试集准确率结果，并与之前的结果对比。 （4分）

**提示：**

1.AFQMC数据集已给出；

2.data.py文件中有数据预处理可能用到的函数可以参考。

3.实现自注意力模型时，掩蔽元素不应参与注意力的计算；

4.本实验不对训练精度做要求

## 实验分析

### 实验核心目标
*   **任务类型**：文本语义匹配，即判断两个句子（Sentence 1 和 Sentence 2）的含义是否相同
*   **数据集**：AFQMC（蚂蚁金融语义相似度数据集）
*   **核心模型**：需要手动搭建一个 **Transformer Encoder**（类似 BERT 的架构，而非直接调用现成的 `bert-base` 库），并外接分类器进行二分类（0或1）

### 具体的步骤与要求
需要完成以下 8 个具体模块：

1.  数据预处理：
    *   使用字级别分词
    *   构建输入格式：`[CLS] 句子1 [SEP] 句子2 [SEP]`（或者类似的结构，需包含占位符和分隔符）
    *   处理 Padding 对齐和 Mini-batch 构建
2.  嵌入层实现：
    *   必须包含三部分相加：Token Embedding (字向量) + Segment Embedding (分段编码) + Position Embedding (位置编码)
3.  核心层实现：
    *   实现 多头自注意力，且必须处理 Padding Mask（掩蔽元素不参与计算）
    *   实现 Add & Norm 层
4.  模型组装：Transformer Encoder + Classifier（全连接层）
5.  训练与监控：
    *   在验证集上保存准确率最高的模型
    *   使用 TensorBoard 可视化 Loss 和 Accuracy 曲线
6.  测试与推理：加载保存的模型，随机抽取 50 条验证集数据进行预测展示
7.  可视化分析：提取 Attention 权重，画出类似文档中提供的 **热力图 (Heatmap)**，并进行分析
8.  对比实验：修改 Transformer 的层数（例如 1层 vs 2层 vs 6层），对比准确率变化

## 实验原理

本实验旨在利用深度学习模型解决**中文文本语义匹配**问题。该任务的本质是一个二分类问题：给定两个句子（Sentence A 和 Sentence B），模型需要判断它们在语义上是否等价（标签为 1 表示相似，0 表示不相似）。

为了处理这一任务，本实验摒弃了传统的循环神经网络（RNN/LSTM），采用了基于**注意力机制**的 Transformer 编码器架构。该架构主要由以下四个关键模块构成：

### 1. 输入表示
与计算机视觉中图像具有固定的像素坐标不同，文本是离散的序列。为了让神经网络理解文本，通过 Transformer 进行语义匹配时，输入层需要融合三种不同的特征向量：

*   字嵌入：将文本中的每个汉字转换成固定维度的稠密向量，捕捉字本身的基础语义
*   位置嵌入：由于 Transformer 内部没有循环结构，它默认无法识别“我看书”和“书看我”的区别。因此，必须显式地加入位置向量，让模型知道每个字在句子中的先后顺序
*   分段嵌入：由于输入是两个句子拼接而成的，需要给第一个句子的所有字打上标记 A，给第二个句子的所有字打上标记 B，以区分不同的句子片段。

最终的输入格式遵循 BERT 的经典范式：`[CLS] 句子1 [SEP] 句子2 [SEP]`，其中 `[CLS]` 是用于分类的特殊标记，`[SEP]` 是分隔符。

### 2. 多头自注意力机制

在处理序列中的每一个字时，**自注意力机制**允许模型看到整个序列中的其他字，并根据相关性分配不同注意力的权重。

**原理**：通过计算查询向量（Query）与键向量（Key）的点积相似度，得到每个字对于其他字的关注程度，再将这些权重作用于值向量（Value）。

**多头**：将向量切分为多个子空间（Head），让模型并行地从不同的角度捕捉特征，类似于 CNN 中使用多个卷积核提取不同特征。例如：一个头关注语法结构，另一个头关注指代关系

**掩码**：由于输入文本长度不一，需要用 0 进行填充（Padding）以对齐批量数据。在计算注意力时，必须引入 Mask 机制，强制模型忽略填充位置，防止无意义的 0 值干扰计算。

### 3. 前馈神经网络与残差连接

在注意力层之后，通过全连接前馈网络进一步提取非线性特征。为了防止随着网络层数加深而出现梯度消失或退化问题，每个子层都引入了**残差连接和层归一化**，确保每一层输出的数值分布稳定，加速模型收敛。

###  4. 任务分类头

经过 $N$ 层 Transformer 编码器的处理后，整个输入序列的信息会被高度压缩和抽象。根据约定，序列首位的特殊标记 `[CLS]` 对应的输出向量被视为整个“句对”的语义表示。

在本实验中，我将 `[CLS]` 位置的输出向量提取出来，输入到一个简单的全连接层中，最后通过 Sigmoid 或 Softmax 函数输出属于“相似”或“不相似”的概率，从而完成语义匹配任务。

---

## 实验步骤

### 1. 数据预处理

为了将 AFQMC 数据集的文本数据转换为模型可接受的 Tensor 格式，本实验编写了 `dataloader.py` 脚本，完成了词表构建、数据集封装及 DataLoader 的配置。具体处理流程如下：

**1. 构建字符级词表**

遍历 train.json 训练集文件，统计所有出现的字符，构建了字符到整数索引（ID）的映射表。初始化 4 个特殊标记：[PAD]（填充，ID=0）、[UNK]（未知字，ID=1）、[CLS]（分类标识，ID=2）、[SEP]（分隔标识，ID=3）。

在 Vocab 类的 `build_vocab` 方法里实现：

```python
"""遍历训练集构建词表"""
with open(data_path, 'r', encoding='utf-8') as f:
    for line in f:
        data = json.loads(line)
        # 将句子 1 和 2 的所有字都加入词表
        text = data['sentence1'] + data['sentence2']
        for char in text:
            if char not in self.str_to_id:
                idx = len(self.str_to_id)
                self.str_to_id[char] = idx
                self.id_to_str[idx] = char
```

**2. 规范化输入格式**

在 AFQMCDataset 类中，对每一条样本（包含 sentence1 和 sentence2）进行了如下处理：
1. 分词：采用字级别分词，将中文句子拆解为字符列表。
2. 拼接：按照 BERT 模型的输入标准，将两个句子拼接为：[CLS] 句子1 [SEP] 句子2 [SEP] 的形式。

```python
tokens = [CLS_TOKEN] + tokens1 + [SEP_TOKEN] + tokens2 + [SEP_TOKEN]
```

**3. 序列对齐与特征生成**

设定最大序列长度 max_len=64，保证输入张量维度一致。

**截断处理**：对于总长度超过 64 的序列，进行截断处理；对于长度不足的序列，使用 [PAD] (ID=0) 在末尾进行填充

```python
if len(tokens1) > max_content_len //2:
    tokens1 = tokens1[:max_content_len // 2]
    tokens2 = tokens2[:max_content_len - len(tokens1)]
else:
    tokens2 = tokens2[:max_content_len - len(tokens1)]
```

**生成 Input IDs**：将字符序列转换为对应的整数索引序列

```python
input_ids = self.vocab.convert_tokens_to_ids(tokens)
```

**生成 Token Type IDs**：用于区分句子边界。将 句子1 及其前后的特殊符号标记为 0，将 句子2 及其结束符号标记为 1，填充部分标记为 0

```python
len_s1 = len(tokens1) + 2 # CLS + SEP
len_s2 = len(tokens2) + 1 # SEP
token_type_ids = [0]*len_s1 + [1]*len_s2
```

**生成 Attention Mask**：构建掩码张量，将真实字符位置标记为 1，填充位置标记为 0，用于后续在 Attention 计算中屏蔽无效信息

```python
attention_mask = [1]*len(input_ids)
```

**4. 批量数据加载**

使用 PyTorch 的 DataLoader 对数据集进行封装，设置 batch_size=64。开启 shuffle=True 打乱训练数据，并验证了数据加载的正确性。

**验证结果：**

```bash
词表构建完成，词汇量大小：1708
--------------------
实验步骤(1) 验证结果：
Input IDs Shape: torch.Size([2, 64])
Attention Mask Shape: torch.Size([2, 64])
Labels: tensor([0, 0])

样本 1 解码演示:
Token IDs: [  2 546 104  32  34  35  76 155  88  24   7  84  85  66   3] ...
Tokens: ['[CLS]', '水', '费', '为', '什', '么', '不', '能', '用', '花', '呗', '支', '付', '了', '[SEP]'] ...
```

- 经统计，本实验构建的词表包含 1708 个字符（含特殊标记）。
- 打印的一条 Mini-batch 数据显示，输入张量形状为 [2, 64]，符合预期。解码后的 Token 序列清晰展示了 [CLS] 水 费... [SEP] 的结构，证明数据预处理逻辑正确。

## 2. 构建嵌入层模型



## 实验结果

## 结果分析
